  Strategic Implementation Plan: Phase 2 + API v2 Migration

  Recommendation: Hybrid Sequential-Parallel Approach

  After deep analysis with Gemini 2.5 Pro, here's the optimal path forward:

  ---
  Executive Summary

  Start with Phase 2 KV→D1 (2 weeks), then layer API v2 on top incrementally.

  Timeline Overview:
  =================

  Week 1-2:   [Phase 2: KV→D1]           Backend only, zero iOS impact
                └─ D1 database foundation ready

  Week 3-4:   [API v2: Barcode (HTTP)]   Backend + iOS parallel
                └─ Users get reliable barcode scans

  Week 5-6:   [API v2: CSV Import (SSE)] Backend + iOS parallel
                └─ Users get real-time progress

  Week 7-10:  [Testing & Deprecation]    Dual-support, WebSocket sunset
                └─ Full migration complete

  Total Duration: 10 weeks
  Risk Level: LOW (incremental, each phase independently valuable)

  ---
  Why This Approach Wins

  1. Technical Synergy

  D1 works for BOTH APIs:
  - Current WebSocket API benefits from D1 immediately (Week 2)
  - Future HTTP/SSE API built on D1 foundation (Week 3+)
  - No need to refactor API v2 later to add D1

  Phase 2 is already fully planned:
  - 2-week execution ready
  - Migration scripts complete
  - Validation suite ready
  - LOW RISK

  2. Incremental Value Delivery

  | Week | Milestone          | User Benefit                                  |
  |------|--------------------|-----------------------------------------------|
  | 2    | D1 database live   | Backend team: complex queries, user library   |
  | 4    | HTTP barcode       | Users: faster scans, no connection drops      |
  | 6    | SSE CSV import     | Users: real-time progress, background support |
  | 10   | Migration complete | Full WebSocket→HTTP/SSE transition            |

  3. Team Efficiency

  Backend Timeline:
  - Week 1-2: Focus on D1 migration (no context switching)
  - Week 3-4: Build barcode enrichment on D1 foundation
  - Week 5-6: Build CSV import with D1 job tracking

  iOS Timeline:
  - Week 1-2: Start API v2 planning (while backend does D1)
  - Week 3-4: Implement HTTP barcode client
  - Week 5-6: Implement native SSE client

  No blocking dependencies! Each team can work in parallel.

  ---
  Detailed Phase Breakdown

  Phase 2A: D1 Foundation (Weeks 1-2)

  Scope: Backend-only, zero iOS changes

  ┌─────────────────────────────────────┐
  │   KV Blob Storage (Current)         │
  │   ↓ Migration Script                │
  │   D1 Relational Database (New)      │
  │   - Books table                     │
  │   - UserLibrary table               │
  │   - CacheMetrics table              │
  └─────────────────────────────────────┘

  Deliverables:
  - D1 schema deployed (Books, UserLibrary, CacheMetrics)
  - KV→D1 migration executed and validated
  - Read-through cache pattern (KV → D1 → External API)
  - User library API endpoints (/v1/library/:userId)
  - Complex query support (JOINs, filters, aggregations)

  Risk: LOW (fully planned, proven pattern, zero user impact)

  ---
  Phase 2B: API v2 Barcode Enrichment (Weeks 3-4)

  Scope: Backend + iOS parallel

  Backend (Week 3):
  // New endpoint on D1 foundation
  POST /api/v2/books/enrich
  {
    "barcode": "9780747532743",
    "idempotency_key": "scan_20250122_abc"
  }

  // Implementation leverages D1:
  1. Check KV cache
  2. Check D1 database (source of truth)
  3. Fetch from Google Books/OpenLibrary
  4. Write to D1 → Write to KV → Return

  iOS (Week 3-4):
  // New service actor
  actor EnrichmentService {
      func enrichBook(barcode: String) async throws -> Book {
          // Native HTTP (no WebSocket!)
          let url = URL(string: "https://api.oooefam.net/api/v2/books/enrich")!
          var request = URLRequest(url: url)
          request.httpMethod = "POST"
          request.setValue("application/json", forHTTPHeaderField: "Content-Type")

          let body = ["barcode": barcode]
          request.httpBody = try JSONEncoder().encode(body)

          let (data, response) = try await URLSession.shared.data(for: request)

          // Handle 404, 429, 503 gracefully
          guard let httpResponse = response as? HTTPURLResponse else {
              throw EnrichmentError.invalidResponse
          }

          switch httpResponse.statusCode {
          case 200:
              return try JSONDecoder().decode(Book.self, from: data)
          case 404:
              throw EnrichmentError.bookNotFound
          case 429:
              throw EnrichmentError.rateLimited
          default:
              throw EnrichmentError.serverError
          }
      }
  }

  Release Strategy:
  - Week 4: Enable for 10% users → Monitor → 50% → 100%
  - Feature flag: useHTTPEnrichment (instant rollback)

  User Benefit: Faster, more reliable barcode scans

  Risk: LOW (simple HTTP request-response)

  ---
  Phase 2C: API v2 CSV Import with SSE (Weeks 5-6)

  Scope: Backend + iOS parallel

  Backend (Week 5):
  CSV Import Flow:
  1. POST /api/v2/imports → Upload CSV → Job created in D1
  2. Queue processes CSV (batch enrichment, parallel)
  3. Durable Object broadcasts progress via SSE
  4. Job completion stored in D1

  SSE Connection:
  Client → Worker → Durable Object (in-memory writers)
                    ↓
                  D1 (event history for reconnection)

  iOS (Week 5-6):
  // Native SSE client (no third-party dependencies)
  actor SSEClient {
      func streamProgress(jobId: String) -> AsyncStream<ImportProgress> {
          AsyncStream { continuation in
              let url = URL(string: "https://api.oooefam.net/api/v2/imports/\(jobId)/stream")!
              var request = URLRequest(url: url)
              request.setValue("text/event-stream", forHTTPHeaderField: "Accept")

              let task = URLSession.shared.dataTask(with: request) { data, response, error in
                  guard let data = data else { return }

                  // Parse SSE events
                  let events = parseSSE(data: data)
                  events.forEach { continuation.yield($0) }
              }

              task.resume()

              continuation.onTermination = { _ in
                  task.cancel()
              }
          }
      }
  }

  // Polling fallback if SSE fails
  actor PollingClient {
      func pollStatus(jobId: String) async throws -> ImportStatus {
          // GET /api/v2/imports/{jobId} every 10s
      }
  }

  Release Strategy:
  - Week 6: Enable SSE for 10% users → Monitor reconnection rate → 100%
  - Automatic fallback to polling if SSE fails 3 times

  User Benefit: Real-time CSV import progress, no connection drops

  Risk: MEDIUM (SSE complexity, mitigated by polling fallback)

  ---
  Phase 2D: Testing & Deprecation (Weeks 7-10)

  Week 7-8: Comprehensive Testing
  - Integration tests (HTTP + SSE + D1 all working together)
  - Load testing (k6 scripts, P95 < 500ms validation)
  - Migration validation (data integrity, cache hit rates)
  - Dual-support monitoring (both APIs live)

  Week 9: Deprecation Notice
  - WebSocket endpoints return deprecation headers
  - In-app banner: "Update to latest version for better performance"
  - Email campaign to users on old app versions

  Week 10: WebSocket Sunset
  - WebSocket endpoints return 410 Gone
  - Remove WebSocket code from backend
  - Clean up dual-support infrastructure
  - Migration complete!

  ---
  Comparison: Why Not The Alternatives?

  Option 1: Backend-First (Phase 2 → API v2 sequential)

  Timeline: 2 weeks (Phase 2) + 7-8 weeks (API v2) = 9-10 weeks

  Problems:
  - Users stuck with WebSocket issues for 9-10 weeks
  - No incremental value delivery
  - Context switching (finish Phase 2, then start API v2)
  - iOS team idle during Phase 2

  Option 2: API v2 First (skip Phase 2)

  Timeline: 7-8 weeks (API v2 only)

  Problems:
  - D1 database delayed 7-8 weeks (no complex queries)
  - Phase 2 planning effort goes stale
  - May need to refactor API v2 later to add D1
  - CSV import jobs stored in KV (less elegant than D1)

  Option 3: Hybrid (Recommended)

  Timeline: 10 weeks total
  Benefits:
  - D1 foundation in 2 weeks
  - Incremental user value (Week 4, Week 6)
  - Lower risk per release
  - Teams work in parallel
  - No refactoring needed

  ---
  Decision Matrix

  | Criteria             | Backend-First | API v2 First | Hybrid      |
  |----------------------|---------------|--------------|-------------|
  | Time to D1 database  | 2 weeks       | 9+ weeks     | 2 weeks     |
  | Time to HTTP barcode | 9 weeks       | 2 weeks      | 4 weeks     |
  | Time to SSE CSV      | 10 weeks      | 6 weeks      | 6 weeks     |
  | Risk per release     | High          | High         | Low         |
  | User value delivery  | Delayed       | Fast         | Incremental |
  | Team coordination    | Sequential    | Sequential   | Parallel    |
  | Technical debt       | Low           | Medium       | Low         |

  ---
  Final Recommendation

  Start Phase 2 KV→D1 migration next week.

  Immediate Next Steps:

  1. This Week:
    - Review Phase 2 KV→D1 plan (already complete)
    - Confirm D1 database naming, wrangler.toml setup
    - Schedule 2-week backend sprint
  2. Week 1-2: Execute Phase 2
    - Run migration script (checkpointed, resumable)
    - Validate data integrity
    - Deploy user library endpoints
    - Zero iOS changes required
  3. Week 3: Start API v2 Barcode
    - Backend: Build POST /api/v2/books/enrich on D1
    - iOS: Start HTTP client implementation
    - Both teams work in parallel

  This gives you:
  - Strong D1 foundation by Week 2
  - User-facing improvements by Week 4
  - Full migration by Week 10
  - Low risk, high value delivery

  ---
  Would you like me to:
  1. Generate implementation checklists for Phase 2A (D1 migration)?
  2. Create the API v2 backend implementation plan for barcode enrichment?
  3. Draft the iOS HTTP client implementation guide?

  Ready to start Phase 2 next week?

> imp checklists for phase 2a is great 

⏺ Phase 2A Implementation Checklist: KV→D1 Migration

  Duration: 2 weeks (Days 1-14)
  Team: Backend Engineering
  Risk Level: LOW
  iOS Impact: ZERO

  ---
  Pre-Flight Checklist (Day 0 - Before Starting)

  Environment Setup

  - Cloudflare Workers account has D1 enabled
  - wrangler CLI installed and authenticated (npx wrangler whoami)
  - Local dev environment has Node.js 18+ and npm
  - Git repository clean (no uncommitted Phase 1 work)
  - Staging environment available for testing

  Team Readiness

  - Backend engineer assigned (primary)
  - QA engineer on standby (testing)
  - DevOps/monitoring setup ready
  - Rollback plan reviewed and approved
  - Stakeholders notified of 2-week migration window

  Documentation Review

  - Read complete Phase 2 plan (docs/migrations/KV_TO_D1_MIGRATION.md)
  - Review D1 documentation (https://developers.cloudflare.com/d1/)
  - Understand rollback procedures
  - Incident response plan in place

  ---
  Days 1-3: Database Schema Design & Setup

  Day 1: D1 Database Creation

  Morning: Create D1 Databases
  # Create local development database
  npx wrangler d1 create bookstrack-db-local

  # Create staging database  
  npx wrangler d1 create bookstrack-db-staging

  # Create production database (for later)
  npx wrangler d1 create bookstrack-db

  - Local database created
  - Staging database created
  - Production database created
  - Database IDs saved to wrangler.toml

  Afternoon: Configure Bindings

  Edit wrangler.toml:
  # Add D1 bindings
  [[d1_databases]]
  binding = "DB"
  database_name = "bookstrack-db-local"
  database_id = "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

  # Keep existing KV binding
  [[kv_namespaces]]
  binding = "BOOK_CACHE"
  id = "your-existing-kv-id"
  preview_id = "your-preview-kv-id"

  # Add idempotency cache
  [[kv_namespaces]]
  binding = "IDEMPOTENCY_CACHE"
  id = "your-idempotency-kv-id"

  - wrangler.toml updated with D1 bindings
  - Local, staging, production configs separated
  - Environment-specific database IDs configured
  - KV namespaces still intact (both will coexist)

  ---
  Day 2: Schema Migration Files

  Create Migration Directory:
  mkdir -p migrations
  touch migrations/0001_initial_schema.sql
  touch migrations/0002_add_user_library.sql
  touch migrations/0003_add_cache_metrics.sql

  File: migrations/0001_initial_schema.sql
  -- Books table (primary data source)
  CREATE TABLE IF NOT EXISTS Books (
    isbn TEXT PRIMARY KEY NOT NULL,
    title TEXT NOT NULL,
    authors TEXT NOT NULL,                    -- JSON array: ["Author 1", "Author 2"]
    authors_detailed TEXT,                    -- JSON objects with full author info
    publication_year INTEGER,
    publisher TEXT,
    page_count INTEGER,
    language TEXT DEFAULT 'en',
    categories TEXT,                          -- JSON array: ["Fiction", "Fantasy"]
    description TEXT,
    cover_url TEXT,
    thumbnail_url TEXT,
    canonical_metadata TEXT NOT NULL,         -- Full JSON blob for backwards compat
    created_at INTEGER NOT NULL DEFAULT (unixepoch()),
    updated_at INTEGER NOT NULL DEFAULT (unixepoch())
  );

  -- Indexes for common queries
  CREATE INDEX IF NOT EXISTS idx_books_year
    ON Books(publication_year);

  CREATE INDEX IF NOT EXISTS idx_books_language
    ON Books(language);

  CREATE INDEX IF NOT EXISTS idx_books_created
    ON Books(created_at);

  CREATE INDEX IF NOT EXISTS idx_books_updated
    ON Books(updated_at);

  - 0001_initial_schema.sql created with Books table
  - Primary key on isbn defined
  - Indexes created for common queries
  - Default values set for timestamps

  File: migrations/0002_add_user_library.sql
  -- UserLibrary table (user collections)
  CREATE TABLE IF NOT EXISTS UserLibrary (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    isbn TEXT NOT NULL,
    status TEXT CHECK(status IN ('want_to_read', 'reading', 'completed')),
    rating INTEGER CHECK(rating >= 1 AND rating <= 5),
    review TEXT,
    added_at INTEGER NOT NULL DEFAULT (unixepoch()),
    started_at INTEGER,
    completed_at INTEGER,
    FOREIGN KEY (isbn) REFERENCES Books(isbn) ON DELETE CASCADE,
    UNIQUE(user_id, isbn)  -- Prevent duplicate entries
  );

  -- Indexes for user queries
  CREATE INDEX IF NOT EXISTS idx_user_library_user
    ON UserLibrary(user_id);

  CREATE INDEX IF NOT EXISTS idx_user_library_status
    ON UserLibrary(user_id, status);

  CREATE INDEX IF NOT EXISTS idx_user_library_rating
    ON UserLibrary(user_id, rating);

  CREATE INDEX IF NOT EXISTS idx_user_library_added
    ON UserLibrary(added_at DESC);

  - 0002_add_user_library.sql created
  - Foreign key constraint to Books table
  - Unique constraint on (user_id, isbn)
  - Status enum validation
  - Rating range validation (1-5)

  File: migrations/0003_add_cache_metrics.sql
  -- CacheMetrics table (observability)
  CREATE TABLE IF NOT EXISTS CacheMetrics (
    cache_key TEXT PRIMARY KEY NOT NULL,
    hit_count INTEGER NOT NULL DEFAULT 0,
    miss_count INTEGER NOT NULL DEFAULT 0,
    last_accessed INTEGER NOT NULL DEFAULT (unixepoch()),
    data_size_bytes INTEGER,
    ttl_seconds INTEGER
  );

  CREATE INDEX IF NOT EXISTS idx_cache_last_accessed
    ON CacheMetrics(last_accessed DESC);

  CREATE INDEX IF NOT EXISTS idx_cache_hit_count
    ON CacheMetrics(hit_count DESC);

  - 0003_add_cache_metrics.sql created
  - Indexes for analytics queries

  ---
  Day 3: Apply Migrations & Validate Schema

  Apply to Local:
  # Apply all migrations locally
  npx wrangler d1 migrations apply bookstrack-db-local --local

  # Verify tables created
  npx wrangler d1 execute bookstrack-db-local --local \
    --command "SELECT name FROM sqlite_master WHERE type='table'"

  # Expected output: Books, UserLibrary, CacheMetrics

  - Migrations applied to local database
  - All 3 tables exist
  - No migration errors

  Apply to Staging:
  # Apply to staging (remote)
  npx wrangler d1 migrations apply bookstrack-db-staging --remote

  # Verify schema
  npx wrangler d1 execute bookstrack-db-staging --remote \
    --command "PRAGMA table_info(Books)"

  # Verify indexes
  npx wrangler d1 execute bookstrack-db-staging --remote \
    --command "SELECT name FROM sqlite_master WHERE type='index'"

  - Migrations applied to staging
  - Schema validated
  - Indexes confirmed

  Test Data Insertion:
  # Insert test book
  npx wrangler d1 execute bookstrack-db-local --local \
    --command "INSERT INTO Books (isbn, title, authors, canonical_metadata) 
               VALUES ('9780439708180', 'Harry Potter', '[\"J.K. Rowling\"]', '{}')"

  # Verify insertion
  npx wrangler d1 execute bookstrack-db-local --local \
    --command "SELECT * FROM Books WHERE isbn = '9780439708180'"

  # Delete test data
  npx wrangler d1 execute bookstrack-db-local --local \
    --command "DELETE FROM Books WHERE isbn = '9780439708180'"

  - Test insertion successful
  - Test query successful
  - Test deletion successful
  - Schema ready for migration

  Day 3 Deliverables:
  - 3 migration files committed to git
  - Local database schema validated
  - Staging database schema validated
  - Documentation updated (docs/DATABASE_SCHEMA.md)

  ---
  Days 4-7: Migration Script Development

  Day 4: Core Migration Logic

  Create Migration Script:
  mkdir -p src/migrations
  touch src/migrations/kv-to-d1-migrator.ts
  touch src/migrations/types.ts

  File: src/migrations/types.ts
  export interface BookRecord {
    isbn: string
    title: string
    authors: string  // JSON string
    authors_detailed: string | null
    publication_year: number | null
    publisher: string | null
    page_count: number | null
    language: string
    categories: string | null  // JSON string
    description: string | null
    cover_url: string | null
    thumbnail_url: string | null
    canonical_metadata: string  // Full JSON blob
  }

  export interface MigrationCheckpoint {
    cursor?: string
    totalMigrated: number
    totalErrors: number
    timestamp: number
    lastProcessedKey?: string
  }

  export interface MigrationResult {
    totalMigrated: number
    totalErrors: number
    duration: number
    errors: Array<{ key: string; error: string }>
  }

  - TypeScript types defined
  - Interfaces match D1 schema

  File: src/migrations/kv-to-d1-migrator.ts
  import { Env } from '../types/env'

  const BATCH_SIZE = 100  // Process 100 keys per iteration
  const MAX_ERRORS_TO_LOG = 100  // Limit error array size

  export async function migrateKVToD1(
    env: Env,
    startCursor?: string
  ): Promise<MigrationResult> {
    const startTime = Date.now()
    let cursor: string | undefined = startCursor
    let totalMigrated = 0
    let totalErrors = 0
    const errors: Array<{ key: string; error: string }> = []

    console.log(`Starting migration from cursor: ${cursor || 'beginning'}`)

    do {
      // List KV keys with pagination
      const result = await env.BOOK_CACHE.list({
        prefix: 'book:isbn:',
        limit: BATCH_SIZE,
        cursor: cursor
      })

      console.log(`Processing batch: ${result.keys.length} keys`)

      // Process batch in parallel
      const batchResult = await processBatch(result.keys, env)
      totalMigrated += batchResult.success
      totalErrors += batchResult.errors.length

      // Collect errors (limit to first 100)
      if (errors.length < MAX_ERRORS_TO_LOG) {
        errors.push(...batchResult.errors.slice(0, MAX_ERRORS_TO_LOG - errors.length))
      }

      // Save checkpoint after each batch
      await saveCheckpoint(env, {
        cursor: result.cursor,
        totalMigrated,
        totalErrors,
        timestamp: Date.now(),
        lastProcessedKey: result.keys[result.keys.length - 1]?.name
      })

      cursor = result.cursor

      // Log progress every batch
      console.log(`Progress: ${totalMigrated} migrated, ${totalErrors} errors`)

    } while (cursor)  // Continue until all keys processed

    const duration = Date.now() - startTime
    console.log(`Migration complete: ${totalMigrated} books in ${duration}ms`)

    return { totalMigrated, totalErrors, duration, errors }
  }

  - Main migration loop implemented
  - Pagination handling
  - Checkpoint saving after each batch
  - Progress logging

  ---
  Day 5: Batch Processing & Data Transformation

  Add to src/migrations/kv-to-d1-migrator.ts:
  async function processBatch(
    keys: KVNamespaceListKey[],
    env: Env
  ): Promise<{ success: number; errors: Array<{ key: string; error: string }> }> {
    let success = 0
    const errors: Array<{ key: string; error: string }> = []

    // Process keys in parallel (but limit concurrency to avoid timeout)
    const promises = keys.map(async (key) => {
      try {
        // Fetch KV value
        const kvData = await env.BOOK_CACHE.get(key.name, 'json')
        if (!kvData) {
          console.warn(`Empty value for key: ${key.name}`)
          return
        }

        // Parse ISBN from key (format: book:isbn:9780439708180)
        const isbn = key.name.split(':')[2]
        if (!isbn || isbn.length < 10) {
          throw new Error(`Invalid ISBN in key: ${key.name}`)
        }

        // Transform to D1 schema
        const bookRecord = transformToD1Schema(kvData, isbn)

        // Upsert to D1 (idempotent)
        await upsertBookToD1(env.DB, bookRecord)

        // Verify write succeeded
        const verified = await verifyBookInD1(env.DB, isbn)
        if (!verified) {
          throw new Error(`Verification failed for ISBN: ${isbn}`)
        }

        success++

      } catch (error) {
        const errorMsg = error instanceof Error ? error.message : String(error)
        errors.push({ key: key.name, error: errorMsg })
        console.error(`Migration failed for ${key.name}:`, errorMsg)

        // Log to D1 error table (optional)
        await logMigrationError(env.DB, key.name, errorMsg).catch(err => {
          console.error('Failed to log error:', err)
        })
      }
    })

    await Promise.all(promises)
    return { success, errors }
  }

  function transformToD1Schema(kvData: any, isbn: string): BookRecord {
    return {
      isbn: isbn,
      title: kvData.title || 'Unknown Title',
      authors: JSON.stringify(kvData.authors || []),
      authors_detailed: kvData.authorsDetailed
        ? JSON.stringify(kvData.authorsDetailed)
        : null,
      publication_year: kvData.publishedDate
        ? parseInt(kvData.publishedDate.substring(0, 4))
        : null,
      publisher: kvData.publisher || null,
      page_count: kvData.pageCount || null,
      language: kvData.language || 'en',
      categories: kvData.categories
        ? JSON.stringify(kvData.categories)
        : null,
      description: kvData.description || null,
      cover_url: kvData.imageLinks?.large || kvData.imageLinks?.medium || null,
      thumbnail_url: kvData.imageLinks?.thumbnail || null,
      canonical_metadata: JSON.stringify(kvData)  // Full blob
    }
  }

  - Batch processing with parallel execution
  - Data transformation logic
  - Error handling and logging
  - Verification after each write

  ---
  Day 6: D1 Operations & Checkpointing

  Add to src/migrations/kv-to-d1-migrator.ts:
  async function upsertBookToD1(db: D1Database, book: BookRecord): Promise<void> {
    const stmt = db.prepare(`
      INSERT INTO Books (
        isbn, title, authors, authors_detailed, publication_year,
        publisher, page_count, language, categories, description,
        cover_url, thumbnail_url, canonical_metadata, created_at, updated_at
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, unixepoch(), unixepoch())
      ON CONFLICT(isbn) DO UPDATE SET
        title = excluded.title,
        authors = excluded.authors,
        authors_detailed = excluded.authors_detailed,
        publication_year = excluded.publication_year,
        publisher = excluded.publisher,
        page_count = excluded.page_count,
        language = excluded.language,
        categories = excluded.categories,
        description = excluded.description,
        cover_url = excluded.cover_url,
        thumbnail_url = excluded.thumbnail_url,
        canonical_metadata = excluded.canonical_metadata,
        updated_at = unixepoch()
    `)

    await stmt.bind(
      book.isbn, book.title, book.authors, book.authors_detailed,
      book.publication_year, book.publisher, book.page_count,
      book.language, book.categories, book.description,
      book.cover_url, book.thumbnail_url, book.canonical_metadata
    ).run()
  }

  async function verifyBookInD1(db: D1Database, isbn: string): Promise<boolean> {
    const result = await db.prepare('SELECT isbn FROM Books WHERE isbn = ?')
      .bind(isbn)
      .first()

    return result !== null
  }

  async function saveCheckpoint(env: Env, checkpoint: MigrationCheckpoint): Promise<void> {
    await env.BOOK_CACHE.put(
      'migration:checkpoint',
      JSON.stringify(checkpoint),
      { expirationTtl: 86400 }  // 24 hour expiry
    )
  }

  async function loadCheckpoint(env: Env): Promise<MigrationCheckpoint | null> {
    const data = await env.BOOK_CACHE.get('migration:checkpoint', 'json')
    return data as MigrationCheckpoint | null
  }

  async function logMigrationError(
    db: D1Database,
    key: string,
    error: string
  ): Promise<void> {
    await db.prepare(`
      INSERT INTO MigrationErrors (cache_key, error_message, occurred_at)
      VALUES (?, ?, unixepoch())
    `).bind(key, error).run()
  }

  - Upsert logic (idempotent)
  - Verification query
  - Checkpoint save/load
  - Error logging to D1

  ---
  Day 7: Admin API Endpoint & Testing

  Create Admin Endpoint:

  File: src/routes/admin.ts
  import { Hono } from 'hono'
  import { migrateKVToD1, loadCheckpoint } from '../migrations/kv-to-d1-migrator'
  import { Env } from '../types/env'

  const admin = new Hono<{ Bindings: Env }>()

  // Admin authentication middleware
  admin.use('*', async (c, next) => {
    const authHeader = c.req.header('Authorization')
    const expectedToken = c.env.ADMIN_TOKEN

    if (!authHeader || !authHeader.startsWith('Bearer ')) {
      return c.json({ error: 'Unauthorized' }, 401)
    }

    const token = authHeader.replace('Bearer ', '')
    if (token !== expectedToken) {
      return c.json({ error: 'Invalid token' }, 403)
    }

    await next()
  })

  // Migration endpoint
  admin.post('/migrate-kv-to-d1', async (c) => {
    const { resume, dryRun } = c.req.query()

    if (dryRun === 'true') {
      // Count KV keys without processing
      let count = 0
      let cursor

      do {
        const result = await c.env.BOOK_CACHE.list({
          prefix: 'book:isbn:',
          limit: 100,
          cursor
        })
        count += result.keys.length
        cursor = result.cursor
      } while (cursor)

      return c.json({
        message: 'Dry run - no data written',
        estimatedKeys: count,
        estimatedDuration: `${Math.ceil(count / 100)} seconds`
      })
    }

    // Load checkpoint if resuming
    let startCursor
    if (resume === 'true') {
      const checkpoint = await loadCheckpoint(c.env)
      startCursor = checkpoint?.cursor
      console.log(`Resuming from checkpoint: ${startCursor}`)
    }

    // Run migration
    const result = await migrateKVToD1(c.env, startCursor)

    return c.json({
      success: true,
      data: result
    })
  })

  // Check migration status
  admin.get('/migration-status', async (c) => {
    const checkpoint = await loadCheckpoint(c.env)

    if (!checkpoint) {
      return c.json({ status: 'not_started' })
    }

    return c.json({
      status: 'in_progress',
      ...checkpoint
    })
  })

  export default admin

  - Admin routes created
  - Authentication middleware
  - Dry-run mode implemented
  - Resume capability
  - Status endpoint

  Add to main router (src/index.ts):
  import admin from './routes/admin'

  app.route('/admin', admin)

  - Admin routes mounted
  - Environment variable ADMIN_TOKEN configured

  Day 7 Testing:
  # Set admin token
  npx wrangler secret put ADMIN_TOKEN
  # Enter: "your-secure-token-here"

  # Test dry run locally
  curl -X POST "http://localhost:8787/admin/migrate-kv-to-d1?dryRun=true" \
    -H "Authorization: Bearer your-secure-token-here"

  # Test actual migration locally (small dataset)
  curl -X POST "http://localhost:8787/admin/migrate-kv-to-d1" \
    -H "Authorization: Bearer your-secure-token-here"

  # Check status
  curl "http://localhost:8787/admin/migration-status" \
    -H "Authorization: Bearer your-secure-token-here"

  - Dry run successful
  - Local migration tested
  - Status endpoint working
  - Authentication enforced

  Day 4-7 Deliverables:
  - Migration script complete and tested locally
  - Admin API endpoints working
  - Dry-run mode validated
  - Resume capability tested
  - Error handling comprehensive

  ---
  Days 8-10: Data Access Layer Refactoring

  Day 8: D1 Service Layer

  Create D1 Book Service:

  File: src/services/d1-book-service.ts
  import { Env } from '../types/env'

  export async function getBookFromD1(db: D1Database, isbn: string) {
    const result = await db.prepare(`
      SELECT
        isbn, title, authors, authors_detailed, publication_year,
        publisher, page_count, language, categories, description,
        cover_url, thumbnail_url, canonical_metadata, created_at
      FROM Books
      WHERE isbn = ?
    `).bind(isbn).first()

    if (!result) return null

    // Transform D1 row back to canonical format
    return {
      isbn: result.isbn,
      title: result.title,
      authors: JSON.parse(result.authors as string),
      authorsDetailed: result.authors_detailed
        ? JSON.parse(result.authors_detailed as string)
        : null,
      publishedDate: result.publication_year?.toString(),
      publisher: result.publisher,
      pageCount: result.page_count,
      language: result.language,
      categories: result.categories ? JSON.parse(result.categories as string) : null,
      description: result.description,
      imageLinks: {
        large: result.cover_url,
        medium: result.cover_url,
        thumbnail: result.thumbnail_url
      },
      // Include full canonical blob for backwards compatibility
      ...JSON.parse(result.canonical_metadata as string)
    }
  }

  export async function saveBookToD1(db: D1Database, book: any): Promise<void> {
    const stmt = db.prepare(`
      INSERT INTO Books (
        isbn, title, authors, authors_detailed, publication_year,
        publisher, page_count, language, categories, description,
        cover_url, thumbnail_url, canonical_metadata
      ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
      ON CONFLICT(isbn) DO UPDATE SET
        title = excluded.title,
        authors = excluded.authors,
        updated_at = unixepoch()
    `)

    await stmt.bind(
      book.isbn,
      book.title,
      JSON.stringify(book.authors || []),
      book.authorsDetailed ? JSON.stringify(book.authorsDetailed) : null,
      book.publishedDate ? parseInt(book.publishedDate.substring(0, 4)) : null,
      book.publisher,
      book.pageCount,
      book.language || 'en',
      book.categories ? JSON.stringify(book.categories) : null,
      book.description,
      book.imageLinks?.large || book.imageLinks?.medium,
      book.imageLinks?.thumbnail,
      JSON.stringify(book)
    ).run()
  }

  export async function getUserBooks(
    db: D1Database,
    userId: string,
    filters?: { status?: string; rating?: number; year?: number }
  ) {
    let query = `
      SELECT b.*, ul.status, ul.rating, ul.added_at
      FROM UserLibrary ul
      JOIN Books b ON ul.isbn = b.isbn
      WHERE ul.user_id = ?
    `
    const params = [userId]

    if (filters?.status) {
      query += ` AND ul.status = ?`
      params.push(filters.status)
    }

    if (filters?.rating) {
      query += ` AND ul.rating >= ?`
      params.push(filters.rating.toString())
    }

    if (filters?.year) {
      query += ` AND b.publication_year = ?`
      params.push(filters.year.toString())
    }

    query += ` ORDER BY ul.added_at DESC`

    const { results } = await db.prepare(query).bind(...params).all()
    return results || []
  }

  - getBookFromD1() implemented
  - saveBookToD1() implemented
  - getUserBooks() with filters implemented
  - TypeScript types correct

  ---
  Day 9: Refactor Book Search Service

  Update: src/services/book-search.ts
  import { Env } from '../types/env'
  import { getBookFromD1, saveBookToD1 } from './d1-book-service'
  import { fetchFromGoogleBooks } from './providers/google-books'

  export async function findByISBN(isbn: string, env: Env) {
    // Layer 1: Check KV cache (fastest)
    const cacheKey = `book:isbn:${isbn}`
    const cached = await env.BOOK_CACHE.get(cacheKey, 'json')

    if (cached) {
      console.log(`Cache hit: ${isbn}`)
      await trackCacheHit(env, cacheKey)  // Metrics
      return { ...cached, _cacheSource: 'kv' }
    }

    // Layer 2: Check D1 database (source of truth)
    console.log(`Cache miss, checking D1: ${isbn}`)
    const d1Book = await getBookFromD1(env.DB, isbn)

    if (d1Book) {
      console.log(`D1 hit: ${isbn}`)

      // Populate KV cache from D1
      await env.BOOK_CACHE.put(cacheKey, JSON.stringify(d1Book), {
        expirationTtl: 86400  // 24 hours
      })

      return { ...d1Book, _cacheSource: 'd1' }
    }

    // Layer 3: Fetch from external API (cache miss)
    console.log(`D1 miss, fetching from external API: ${isbn}`)
    const externalBook = await fetchFromGoogleBooks(isbn, env)

    // Write to D1 first (source of truth)
    await saveBookToD1(env.DB, externalBook)

    // Then populate KV cache
    await env.BOOK_CACHE.put(cacheKey, JSON.stringify(externalBook), {
      expirationTtl: 86400
    })

    return { ...externalBook, _cacheSource: 'external_api' }
  }

  async function trackCacheHit(env: Env, cacheKey: string): Promise<void> {
    // Update CacheMetrics table
    await env.DB.prepare(`
      INSERT INTO CacheMetrics (cache_key, hit_count, last_accessed)
      VALUES (?, 1, unixepoch())
      ON CONFLICT(cache_key) DO UPDATE SET
        hit_count = hit_count + 1,
        last_accessed = unixepoch()
    `).bind(cacheKey).run()
  }

  - Read-through cache pattern implemented
  - KV → D1 → External API flow
  - Cache metrics tracking
  - Source attribution (_cacheSource)

  ---
  Day 10: User Library API Endpoints

  Create User Library Routes:

  File: src/routes/user-library.ts
  import { Hono } from 'hono'
  import { getUserBooks } from '../services/d1-book-service'
  import { Env } from '../types/env'

  const library = new Hono<{ Bindings: Env }>()

  // Get user's book collection
  library.get('/:userId', async (c) => {
    const userId = c.req.param('userId')
    const { status, rating, year } = c.req.query()

    const books = await getUserBooks(c.env.DB, userId, {
      status,
      rating: rating ? parseInt(rating) : undefined,
      year: year ? parseInt(year) : undefined
    })

    return c.json({
      success: true,
      data: { books, count: books.length }
    })
  })

  // Add book to user library
  library.post('/:userId/books', async (c) => {
    const userId = c.req.param('userId')
    const { isbn, status, rating, review } = await c.req.json()

    await c.env.DB.prepare(`
      INSERT INTO UserLibrary (user_id, isbn, status, rating, review)
      VALUES (?, ?, ?, ?, ?)
      ON CONFLICT(user_id, isbn) DO UPDATE SET
        status = excluded.status,
        rating = excluded.rating,
        review = excluded.review
    `).bind(userId, isbn, status, rating, review).run()

    return c.json({
      success: true,
      data: { added: true }
    })
  })

  // Update book status
  library.patch('/:userId/books/:isbn', async (c) => {
    const userId = c.req.param('userId')
    const isbn = c.req.param('isbn')
    const updates = await c.req.json()

    const fields: string[] = []
    const values: any[] = []

    if (updates.status) {
      fields.push('status = ?')
      values.push(updates.status)
    }

    if (updates.rating) {
      fields.push('rating = ?')
      values.push(updates.rating)
    }

    if (updates.review !== undefined) {
      fields.push('review = ?')
      values.push(updates.review)
    }

    if (fields.length === 0) {
      return c.json({ error: 'No fields to update' }, 400)
    }

    values.push(userId, isbn)

    await c.env.DB.prepare(`
      UPDATE UserLibrary
      SET ${fields.join(', ')}
      WHERE user_id = ? AND isbn = ?
    `).bind(...values).run()

    return c.json({
      success: true,
      data: { updated: true }
    })
  })

  // Complex query: Top-rated books of year
  library.get('/:userId/top-rated', async (c) => {
    const userId = c.req.param('userId')
    const year = c.req.query('year') || new Date().getFullYear().toString()

    const books = await c.env.DB.prepare(`
      SELECT b.*, ul.rating, ul.review
      FROM UserLibrary ul
      JOIN Books b ON ul.isbn = b.isbn
      WHERE ul.user_id = ?
        AND ul.rating >= 4
        AND b.publication_year = ?
      ORDER BY ul.rating DESC, b.title ASC
    `).bind(userId, parseInt(year)).all()

    return c.json({
      success: true,
      data: { books: books.results }
    })
  })

  export default library

  - GET /v1/library/:userId implemented
  - POST /v1/library/:userId/books implemented
  - PATCH /v1/library/:userId/books/:isbn implemented
  - Complex query endpoint implemented

  Add to main router:
  import library from './routes/user-library'

  app.route('/v1/library', library)

  - User library routes mounted
  - All endpoints tested locally

  Day 8-10 Deliverables:
  - D1 service layer complete
  - Book search refactored with read-through cache
  - User library API endpoints working
  - Complex queries validated
  - All tests passing locally

  ---
  Days 11-12: Testing & Validation

  Day 11: Unit & Integration Tests

  Create Test Suite:
  mkdir -p __tests__/services
  touch __tests__/services/d1-book-service.test.ts
  touch __tests__/integration/cache-layer.test.ts

  File: __tests__/services/d1-book-service.test.ts
  import { describe, it, expect, beforeEach } from 'vitest'
  import { unstable_dev } from 'wrangler'
  import { getBookFromD1, saveBookToD1 } from '../../src/services/d1-book-service'

  describe('D1 Book Service', () => {
    let worker: any

    beforeEach(async () => {
      worker = await unstable_dev('src/index.ts', {
        experimental: { disableExperimentalWarning: true }
      })
    })

    afterEach(async () => {
      await worker.stop()
    })

    it('should return null for non-existent ISBN', async () => {
      const env = await worker.env
      const result = await getBookFromD1(env.DB, '9999999999999')
      expect(result).toBeNull()
    })

    it('should save and retrieve book', async () => {
      const env = await worker.env

      const testBook = {
        isbn: '9780439708180',
        title: 'Harry Potter',
        authors: ['J.K. Rowling'],
        pageCount: 309,
        language: 'en'
      }

      await saveBookToD1(env.DB, testBook)
      const retrieved = await getBookFromD1(env.DB, '9780439708180')

      expect(retrieved).toBeTruthy()
      expect(retrieved.title).toBe('Harry Potter')
      expect(retrieved.authors).toEqual(['J.K. Rowling'])
    })
  })

  - D1 service tests written
  - Save/retrieve cycle tested
  - Edge cases covered

  Run tests:
  npm test

  - All unit tests passing
  - Coverage >75%

  ---
  Day 12: Data Integrity & Load Testing

  Migration Validation Script:

  File: scripts/validate-migration.ts
  import { Env } from '../src/types/env'

  export async function validateMigration(env: Env) {
    console.log('Starting migration validation...')

    // 1. Count KV keys
    let kvCount = 0
    let cursor

    do {
      const result = await env.BOOK_CACHE.list({
        prefix: 'book:isbn:',
        limit: 100,
        cursor
      })
      kvCount += result.keys.length
      cursor = result.cursor
    } while (cursor)

    console.log(`KV keys found: ${kvCount}`)

    // 2. Count D1 records
    const d1Result = await env.DB.prepare('SELECT COUNT(*) as count FROM Books').first()
    const d1Count = d1Result?.count || 0

    console.log(`D1 records found: ${d1Count}`)

    // 3. Compare counts
    const difference = Math.abs(kvCount - d1Count)
    const tolerance = Math.ceil(kvCount * 0.01)  // 1% tolerance

    if (difference > tolerance) {
      throw new Error(`Data mismatch: KV=${kvCount}, D1=${d1Count}, diff=${difference}`)
    }

    console.log(`✓ Counts match within tolerance (diff: ${difference})`)

    // 4. Sample validation
    const sampleSize = Math.min(100, kvCount)
    console.log(`Validating ${sampleSize} random samples...`)

    let samplesChecked = 0
    let sampleErrors = 0

    cursor = undefined
    let remaining = sampleSize

    do {
      const result = await env.BOOK_CACHE.list({
        prefix: 'book:isbn:',
        limit: Math.min(100, remaining),
        cursor
      })

      for (const key of result.keys) {
        const isbn = key.name.split(':')[2]

        const kvData = await env.BOOK_CACHE.get(key.name, 'json')
        const d1Data = await env.DB.prepare('SELECT * FROM Books WHERE isbn = ?')
          .bind(isbn).first()

        if (!d1Data) {
          console.error(`Missing in D1: ${isbn}`)
          sampleErrors++
        } else if (kvData.title !== d1Data.title) {
          console.error(`Title mismatch for ${isbn}`)
          sampleErrors++
        }

        samplesChecked++
        remaining--
      }

      cursor = result.cursor
    } while (cursor && remaining > 0)

    console.log(`✓ Samples validated: ${samplesChecked}, errors: ${sampleErrors}`)

    if (sampleErrors > samplesChecked * 0.01) {
      throw new Error(`Too many sample errors: ${sampleErrors}/${samplesChecked}`)
    }

    console.log('✓ Migration validation passed!')
    return { kvCount, d1Count, samplesChecked, sampleErrors }
  }

  - Validation script created
  - Count comparison logic
  - Sample validation logic

  Load Testing with k6:

  File: load-tests/search-performance.js
  import http from 'k6/http'
  import { check, sleep } from 'k6'

  export const options = {
    stages: [
      { duration: '30s', target: 50 },
      { duration: '1m', target: 100 },
      { duration: '30s', target: 0 }
    ],
    thresholds: {
      http_req_duration: ['p(95)<500'],
      http_req_failed: ['rate<0.01']
    }
  }

  const testISBNs = [
    '9780439708180',
    '9780451524935',
    '9780061120084'
  ]

  export default function() {
    const isbn = testISBNs[Math.floor(Math.random() * testISBNs.length)]

    const res = http.get(`http://localhost:8787/v1/search/isbn?isbn=${isbn}`)

    check(res, {
      'status is 200': (r) => r.status === 200,
      'response time < 500ms': (r) => r.timings.duration < 500,
      'has book data': (r) => JSON.parse(r.body).data.book !== null
    })

    sleep(1)
  }

  - k6 script created
  - Performance thresholds defined

  Run load test:
  k6 run load-tests/search-performance.js

  - Load test passing
  - P95 < 500ms
  - Error rate < 1%

  Day 11-12 Deliverables:
  - Unit tests passing (>75% coverage)
  - Integration tests passing
  - Data integrity validated
  - Load testing successful
  - Performance benchmarks met

  ---
  Day 13: Production Deployment

  Morning: Staging Deployment

  Deploy to Staging:
  # Update wrangler.toml with staging D1 database ID
  # (already done on Day 1)

  # Deploy to staging
  npx wrangler deploy --env staging

  # Verify health
  curl https://staging-api.oooefam.net/health

  - Staging deployment successful
  - Health check passing

  Run Staging Migration:
  # Dry run first
  curl -X POST "https://staging-api.oooefam.net/admin/migrate-kv-to-d1?dryRun=true" \
    -H "Authorization: Bearer $ADMIN_TOKEN"

  # Real migration
  curl -X POST "https://staging-api.oooefam.net/admin/migrate-kv-to-d1" \
    -H "Authorization: Bearer $ADMIN_TOKEN"

  # Monitor progress
  watch -n 5 'curl -s https://staging-api.oooefam.net/admin/migration-status \
    -H "Authorization: Bearer $ADMIN_TOKEN" | jq'

  - Staging migration completed
  - No errors reported

  Validate Staging:
  # Run validation script
  npx wrangler dev --remote --env staging
  # (then run validation in console)

  # Test user library endpoints
  curl "https://staging-api.oooefam.net/v1/library/testuser"
  curl "https://staging-api.oooefam.net/v1/library/testuser?status=reading&rating=5"

  - Staging validation passed
  - User library working
  - Complex queries working

  ---
  Afternoon: Production Deployment

  Production Checklist:
  - Staging migration successful
  - All tests passing
  - Rollback plan reviewed
  - Team on standby

  Deploy to Production:
  # Update wrangler.toml with production D1 database ID

  # Deploy (D1 reads disabled initially)
  npx wrangler deploy

  # Verify deployment
  curl https://api.oooefam.net/health

  - Production code deployed
  - D1 reads still disabled (feature flag)

  Run Production Migration:
  # Backup KV (safety)
  npx wrangler kv:bulk get BOOK_CACHE --namespace-id=$KV_ID > kv-backup-$(date +%Y%m%d).json
  sha256sum kv-backup-*.json > backup-checksum.txt

  # Dry run
  curl -X POST "https://api.oooefam.net/admin/migrate-kv-to-d1?dryRun=true" \
    -H "Authorization: Bearer $ADMIN_TOKEN"

  # Real migration
  curl -X POST "https://api.oooefam.net/admin/migrate-kv-to-d1" \
    -H "Authorization: Bearer $ADMIN_TOKEN"

  # Monitor (keep this running)
  watch -n 10 'curl -s https://api.oooefam.net/admin/migration-status \
    -H "Authorization: Bearer $ADMIN_TOKEN" | jq'

  - KV backup created
  - Production migration started
  - Progress monitored

  Enable D1 Reads (Gradual):
  # Option: Feature flag via KV
  npx wrangler kv:key put --namespace-id=$KV_ID \
    "feature:enable_d1_reads" "true"

  # Monitor for 30 minutes
  npx wrangler tail --format json | grep "error"

  - D1 reads enabled
  - No error spike observed
  - Latency acceptable

  ---
  Evening: Monitoring & Validation

  Real-Time Monitoring:
  # Terminal 1: Tail logs
  npx wrangler tail --format json

  # Terminal 2: Watch metrics
  watch -n 10 'curl -s https://api.oooefam.net/metrics'

  # Terminal 3: Smoke tests
  while true; do
    curl -s https://api.oooefam.net/v1/search/isbn?isbn=9780439708180 | jq '.success'
    sleep 5
  done

  - Error rate <0.1%
  - Latency P95 <500ms
  - Cache hit rate >80%

  Day 13 Deliverables:
  - Production migration complete
  - D1 reads enabled
  - No data loss
  - Performance acceptable
  - Monitoring in place

  ---
  Day 14: Post-Migration Review

  Validation & Cleanup

  Run Full Validation:
  # Validate data integrity
  npm run validate-migration

  # Run regression tests
  npm test -- __tests__/regression

  # Check metrics
  curl https://api.oooefam.net/metrics

  - Data integrity confirmed
  - Regression tests passing
  - Metrics healthy

  Documentation Updates:
  # Update docs
  touch docs/DATABASE_SCHEMA.md
  touch docs/deployment/D1_DEPLOYMENT.md
  touch docs/operations/D1_RUNBOOK.md

  - DATABASE_SCHEMA.md created
  - D1_DEPLOYMENT.md created
  - D1_RUNBOOK.md created
  - API contract updated

  ---
  Success Criteria Validation

  Functional:
  - All book metadata in D1
  - Complex queries working (JOINs, filters)
  - D1 is source of truth (writes go D1-first)
  - Read-through cache pattern working
  - User library endpoints operational

  Performance:
  - Error rate <0.1%
  - Latency P95 <500ms
  - Cache hit rate >80%
  - D1 query duration <100ms

  Operational:
  - Zero data loss
  - Rollback plan tested
  - Monitoring dashboards configured
  - Team trained on new architecture

  ---
  Lessons Learned

  Document:
  - What went well
  - What could be improved
  - Unexpected challenges
  - Recommendations for future migrations

  Post-Mortem Template:
  # Phase 2: KV→D1 Migration Post-Mortem

  ## Overview
  - Duration: 14 days
  - Records migrated: X
  - Downtime: 0 minutes

  ## Successes
  - [List what worked well]

  ## Challenges
  - [List difficulties encountered]

  ## Metrics
  - Migration speed: X records/second
  - Error rate: X%
  - Performance impact: X%

  ## Recommendations
  - [Actionable items for next migration]

  - Post-mortem document created
  - Lessons learned documented
  - Team retrospective held

  ---
  Final Deliverables Checklist

  Code & Configuration

  - D1 database schema in production
  - Migration scripts committed to git
  - D1 service layer implemented
  - Book search refactored
  - User library API endpoints live
  - Admin migration endpoint secured
  - Feature flags in place

  Testing

  - Unit tests (>75% coverage)
  - Integration tests passing
  - Load tests successful
  - Data integrity validated
  - Regression tests passing

  Documentation

  - DATABASE_SCHEMA.md
  - D1_DEPLOYMENT.md
  - D1_RUNBOOK.md
  - API contract updated
  - Post-mortem completed

  Operations

  - Monitoring dashboards
  - Alerting configured
  - Rollback plan documented
  - Team trained
  - Stakeholders notified

  ---
  Emergency Contacts

  If Issues Arise:
  - Tech Lead: [Name/Slack]
  - DevOps: [Name/Slack]
  - Cloudflare Support: [Link]

  Rollback Command:
  # Instant rollback (disable D1 reads)
  npx wrangler kv:key put --namespace-id=$KV_ID \
    "feature:enable_d1_reads" "false"